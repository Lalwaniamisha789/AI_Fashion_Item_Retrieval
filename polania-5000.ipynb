{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10949552,"sourceType":"datasetVersion","datasetId":6808248},{"sourceId":345767,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":288895,"modelId":309641}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfile_paths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_paths.append(os.path.join(dirname, filename))\nprint(f\"Total files found: {len(file_paths)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:03:32.685556Z","iopub.execute_input":"2025-04-19T12:03:32.685843Z","iopub.status.idle":"2025-04-19T12:07:20.103953Z","shell.execute_reply.started":"2025-04-19T12:03:32.685823Z","shell.execute_reply":"2025-04-19T12:07:20.103278Z"}},"outputs":[{"name":"stdout","text":"Total files found: 261081\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\nif torch.cuda.is_available():\n    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU is not available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:07:42.413908Z","iopub.execute_input":"2025-04-19T12:07:42.414406Z","iopub.status.idle":"2025-04-19T12:07:46.093266Z","shell.execute_reply.started":"2025-04-19T12:07:42.414381Z","shell.execute_reply":"2025-04-19T12:07:46.092483Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU is available: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport random\nfrom itertools import combinations, product\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:07:48.579453Z","iopub.execute_input":"2025-04-19T12:07:48.580196Z","iopub.status.idle":"2025-04-19T12:07:52.311607Z","shell.execute_reply.started":"2025-04-19T12:07:48.580169Z","shell.execute_reply":"2025-04-19T12:07:52.311045Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ---------- CONFIG ----------\nROOT = Path('/kaggle/input/polyvore-outfit-dataset/polyvore_outfits')\nTRAIN_FILE = ROOT / 'nondisjoint/train.json'\nSPLIT_RATIO = 0.2\nNEG_MULTIPLIER = 6\nOUTPUT_DIR = Path('/kaggle/working')\ntest_json = ROOT / 'nondisjoint/test.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:07:54.994006Z","iopub.execute_input":"2025-04-19T12:07:54.994760Z","iopub.status.idle":"2025-04-19T12:07:54.998713Z","shell.execute_reply.started":"2025-04-19T12:07:54.994735Z","shell.execute_reply":"2025-04-19T12:07:54.998040Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ---------- LOAD TRAIN.JSON ----------\nwith open(TRAIN_FILE, 'r') as f:\n    outfit_dicts = json.load(f)\n\noutfit_dicts = outfit_dicts[:5000]\n\nprint(f\"Total outfits: {len(outfit_dicts)}\")\nprint(\"First outfit example:\", outfit_dicts[0])\n\n# ---------- Extract list of item_ids per outfit ----------\noutfits = [\n    [item[\"item_id\"] for item in outfit[\"items\"] if \"item_id\" in item]\n    for outfit in outfit_dicts\n    if len(outfit[\"items\"]) >= 2  # filter to avoid empty/1-item outfits\n]\n\n# ---------- SPLIT 80-20 ----------\ntrain_outfits, val_outfits = train_test_split(outfits, test_size=SPLIT_RATIO, random_state=42)\n\n# ---------- HELPER: GET POSITIVE PAIRS ----------\ndef generate_positive_pairs(outfits):\n    pos_pairs = []\n    for item_ids in outfits:\n        for i in range(len(item_ids)):\n            for j in range(i + 1, len(item_ids)):\n                pos_pairs.append((item_ids[i], item_ids[j], 1))\n    return pos_pairs\n\n# ---------- HELPER: GET NEGATIVE PAIRS ----------\ndef generate_negative_pairs(pos_pairs, outfits, n_needed):\n    all_ids = list({item for outfit in outfits for item in outfit})\n    pos_set = set((a, b) for a, b, _ in pos_pairs)\n\n    neg_pairs = set()\n    while len(neg_pairs) < n_needed:\n        a, b = random.sample(all_ids, 2)\n        if a != b and (a, b) not in pos_set and (b, a) not in pos_set:\n            neg_pairs.add((a, b, 0))\n    return list(neg_pairs)\n\n# ---------- GENERATE PAIRS ----------\ntrain_pos = generate_positive_pairs(train_outfits)\nval_pos = generate_positive_pairs(val_outfits)\n\ntrain_neg = generate_negative_pairs(train_pos, train_outfits, len(train_pos) * NEG_MULTIPLIER)\nval_neg = generate_negative_pairs(val_pos, val_outfits, len(val_pos) * NEG_MULTIPLIER)\n\n# ---------- FINAL COMBINED DATA ----------\ntrain_data = train_pos + train_neg\nval_data = val_pos + val_neg\nrandom.shuffle(train_data)\nrandom.shuffle(val_data)\n\n# ---------- SAVE TO FILES ----------\nwith open(OUTPUT_DIR / 'train_pairs.json', 'w') as f:\n    json.dump(train_data, f)\n\nwith open(OUTPUT_DIR / 'val_pairs.json', 'w') as f:\n    json.dump(val_data, f)\n\n# ---------- SUMMARY ----------\nprint(f\"Train pairs saved to: {OUTPUT_DIR / 'train_pairs.json'}\")\nprint(f\"   Total: {len(train_data)} | Positives: {len(train_pos)} | Negatives: {len(train_neg)}\")\nprint(f\"Validation pairs saved to: {OUTPUT_DIR / 'val_pairs.json'}\")\nprint(f\"   Total: {len(val_data)} | Positives: {len(val_pos)} | Negatives: {len(val_neg)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:07:57.977050Z","iopub.execute_input":"2025-04-19T12:07:57.977379Z","iopub.status.idle":"2025-04-19T12:08:00.619323Z","shell.execute_reply.started":"2025-04-19T12:07:57.977356Z","shell.execute_reply":"2025-04-19T12:08:00.618653Z"}},"outputs":[{"name":"stdout","text":"Total outfits: 5000\nFirst outfit example: {'items': [{'item_id': '154249722', 'index': 1}, {'item_id': '188425631', 'index': 2}, {'item_id': '183214727', 'index': 3}], 'set_id': '210750761'}\nTrain pairs saved to: /kaggle/working/train_pairs.json\n   Total: 362145 | Positives: 51735 | Negatives: 310410\nValidation pairs saved to: /kaggle/working/val_pairs.json\n   Total: 89733 | Positives: 12819 | Negatives: 76914\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class FashionPairDataset(Dataset):\n    def __init__(self, pair_data, image_dir, transform=None):\n        self.pairs = pair_data\n        self.image_dir = image_dir\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        item1, item2, label = self.pairs[idx]\n        img1 = Image.open(f\"{self.image_dir}/{item1}.jpg\").convert('RGB')\n        img2 = Image.open(f\"{self.image_dir}/{item2}.jpg\").convert('RGB')\n        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:07.289990Z","iopub.execute_input":"2025-04-19T12:08:07.290304Z","iopub.status.idle":"2025-04-19T12:08:07.295628Z","shell.execute_reply.started":"2025-04-19T12:08:07.290281Z","shell.execute_reply":"2025-04-19T12:08:07.295043Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"with open('/kaggle/working/train_pairs.json') as f:\n    train_data = json.load(f)\nwith open('/kaggle/working/val_pairs.json') as f:\n    val_data = json.load(f)\n\ndataset_root = '/kaggle/input/polyvore-outfit-dataset/polyvore_outfits'\nimg_dir = os.path.join(dataset_root, 'images')\n\ntrain_loader = DataLoader(FashionPairDataset(train_data, img_dir), batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(FashionPairDataset(val_data, img_dir), batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:10.679852Z","iopub.execute_input":"2025-04-19T12:08:10.680401Z","iopub.status.idle":"2025-04-19T12:08:11.664566Z","shell.execute_reply.started":"2025-04-19T12:08:10.680376Z","shell.execute_reply":"2025-04-19T12:08:11.664007Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_truncated_vgg16():\n    vgg = models.vgg16(pretrained=True)\n    child_layers = list(vgg.features.children())\n    truncated = nn.Sequential(*child_layers[:31])  # Up to conv4_1\n\n    # Freeze first 10 conv layers â†’ up to layer index 22 (conv4_3)\n    conv_count = 0\n    for layer in child_layers:\n        if isinstance(layer, nn.Conv2d):\n            conv_count += 1\n        if conv_count <= 10:\n            for param in layer.parameters():\n                param.requires_grad = False\n\n    return truncated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:15.494259Z","iopub.execute_input":"2025-04-19T12:08:15.494830Z","iopub.status.idle":"2025-04-19T12:08:15.499221Z","shell.execute_reply.started":"2025-04-19T12:08:15.494808Z","shell.execute_reply":"2025-04-19T12:08:15.498460Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntruncated_vgg = get_truncated_vgg16().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:20.329469Z","iopub.execute_input":"2025-04-19T12:08:20.329755Z","iopub.status.idle":"2025-04-19T12:08:25.028118Z","shell.execute_reply.started":"2025-04-19T12:08:20.329734Z","shell.execute_reply":"2025-04-19T12:08:25.027241Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:02<00:00, 209MB/s] \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def extract_color_histogram(img_tensor, bins=8):\n    \"\"\"\n    img_tensor: [3, H, W], range [0,1]\n    Returns: histogram vector of shape [24]\n    \"\"\"\n    hist = []\n    for ch in img_tensor:\n        h = torch.histc(ch.cpu(), bins=bins, min=0.0, max=1.0)\n        h = h / (h.sum() + 1e-6)  # Normalize\n        hist.append(h)\n    return torch.cat(hist).to(img_tensor.device)  # return to original device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:26.853803Z","iopub.execute_input":"2025-04-19T12:08:26.854503Z","iopub.status.idle":"2025-04-19T12:08:26.858559Z","shell.execute_reply.started":"2025-04-19T12:08:26.854479Z","shell.execute_reply":"2025-04-19T12:08:26.857847Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# -------- Siamese Merge Module --------\nclass SiameseMerge(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vgg = get_truncated_vgg16()\n\n    def forward(self, img1, img2):\n        \"\"\"\n        img1, img2: [B, 3, H, W], values in [0,1]\n        \"\"\"\n        feat1 = self.vgg(img1)  # [B, 512, H', W']\n        feat2 = self.vgg(img2)\n        # print(\"VGG output shape:\", feat1.shape)\n        # print(\"VGG output shape:\", feat2.shape)\n        \n        # Global average pool\n        feat1 = F.adaptive_avg_pool2d(feat1, (1, 1)).squeeze(-1).squeeze(-1)  # [B, 512]\n        feat2 = F.adaptive_avg_pool2d(feat2, (1, 1)).squeeze(-1).squeeze(-1)  # [B, 512]\n\n        visual_hadamard = feat1 * feat2  # [B, 512]\n\n        # Compute color histogram Hadamard\n        batch_hist = []\n        for i in range(img1.size(0)):\n            h1 = extract_color_histogram(img1[i])\n            h2 = extract_color_histogram(img2[i])\n            batch_hist.append(h1 * h2)  # [24]\n        hist_hadamard = torch.stack(batch_hist).to(img1.device)  # [B, 24]\n        # print(\"Color hist shape:\", h1.shape)\n        # print(\"Color hist shape:\", h2.shape)\n        # Final feature\n        merged_feature = torch.cat([visual_hadamard, hist_hadamard], dim=1)  # [B, 536]\n        # print(\"Merged feature shape from siamese_merge:\", merged_feature.shape)\n        return merged_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:29.954298Z","iopub.execute_input":"2025-04-19T12:08:29.955015Z","iopub.status.idle":"2025-04-19T12:08:29.960924Z","shell.execute_reply.started":"2025-04-19T12:08:29.954992Z","shell.execute_reply":"2025-04-19T12:08:29.960132Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = SiameseMerge().to(device)\ndummy_img1 = torch.randn(2, 3, 224, 224).to(device)\ndummy_img2 = torch.randn(2, 3, 224, 224).to(device)\n\nwith torch.no_grad():\n    output = model(dummy_img1, dummy_img2)\n\nprint(\">>> Final merged feature dim:\", output.shape[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:33.159488Z","iopub.execute_input":"2025-04-19T12:08:33.160367Z","iopub.status.idle":"2025-04-19T12:08:35.472782Z","shell.execute_reply.started":"2025-04-19T12:08:33.160335Z","shell.execute_reply":"2025-04-19T12:08:35.471954Z"}},"outputs":[{"name":"stdout","text":">>> Final merged feature dim: 536\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class MetricNetwork(nn.Module):\n    def __init__(self, input_dim=536, dropout_prob=0.5):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.dropout1 = nn.Dropout(dropout_prob)\n\n        self.fc2 = nn.Linear(256, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.dropout2 = nn.Dropout(dropout_prob)\n\n        self.fc3 = nn.Linear(64, 1)  # Output scalar compatibility score\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        score = self.fc3(x)  # Raw score (logit)\n        return score.squeeze(1)  # [B]\n\n    def predict(self, x):\n        \"\"\"\n        Inference method to get probabilities (after sigmoid)\n        \"\"\"\n        with torch.no_grad():\n            logits = self.forward(x)\n            probs = torch.sigmoid(logits)\n        return probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:39.929490Z","iopub.execute_input":"2025-04-19T12:08:39.930003Z","iopub.status.idle":"2025-04-19T12:08:39.935789Z","shell.execute_reply.started":"2025-04-19T12:08:39.929977Z","shell.execute_reply":"2025-04-19T12:08:39.935145Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def update_lambda(W, eps=1e-5):\n    norm = torch.norm(W).clamp(min=eps)\n    return torch.eye(W.shape[1], device=W.device) * norm.item() ** 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:42.824744Z","iopub.execute_input":"2025-04-19T12:08:42.825442Z","iopub.status.idle":"2025-04-19T12:08:42.829817Z","shell.execute_reply.started":"2025-04-19T12:08:42.825420Z","shell.execute_reply":"2025-04-19T12:08:42.828885Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class MAPLoss(nn.Module):\n    def __init__(self, metric_model, vgg_model, lambda1=1e-4, lambda2=1e-5, lambda3=1e-5):\n        super().__init__()\n        self.metric_model = metric_model\n        self.vgg_model = vgg_model\n        self.lambda1 = lambda1\n        self.lambda2 = lambda2\n        self.lambda3 = lambda3\n\n    def forward(self, pred_logits, targets, lambdas):\n        pred_logits = pred_logits.clamp(min=-100, max=100)  # Prevents inf logits\n        bce = F.binary_cross_entropy_with_logits(pred_logits, targets.float())\n\n        # Matrix-variate Gaussian prior\n        tr_terms = []\n        for fc_layer, key in zip([self.metric_model.fc1, self.metric_model.fc2], ['fc1', 'fc2']):\n            Wj = fc_layer.weight\n            Lambda_inv = torch.linalg.pinv(lambdas[key].clamp(min=1e-6, max=1e6))  # Regularized inverse\n            Wj_T = Wj.transpose(0, 1)\n            mat = Wj @ Lambda_inv @ Wj_T\n            tr = torch.trace(mat.clamp(min=1e-6, max=1e6))  # Prevent exploding trace\n            tr_terms.append(tr)\n        trace_reg = sum(tr_terms)\n\n        l1_cnn = sum(torch.sum(torch.abs(p)) for n, p in self.vgg_model.named_parameters() if p.requires_grad)\n        l1_w = torch.sum(torch.abs(self.metric_model.fc3.weight))\n\n        total_loss = (\n            bce +\n            self.lambda1 * trace_reg +\n            self.lambda2 * l1_cnn +\n            self.lambda3 * l1_w\n        )\n        return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:45.289429Z","iopub.execute_input":"2025-04-19T12:08:45.289701Z","iopub.status.idle":"2025-04-19T12:08:45.297032Z","shell.execute_reply.started":"2025-04-19T12:08:45.289672Z","shell.execute_reply":"2025-04-19T12:08:45.296212Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_model(siamese_merge, metric_model, loss_fn, optimizer,\n                train_loader, val_loader, device, num_epochs=20, patience=3):\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        siamese_merge.train()\n        metric_model.train()\n        total_train_loss = 0.0\n\n        for img1, img2, labels in tqdm(train_loader, desc=\"Training\"):\n            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            merged_feats = siamese_merge(img1, img2)\n            logits = metric_model(merged_feats)\n\n            # NaN check\n            if torch.isnan(merged_feats).any() or torch.isnan(logits).any():\n                print(\"NaN detected in features or logits. Skipping batch.\")\n                continue\n\n            lambdas = {\n                \"fc1\": update_lambda(metric_model.fc1.weight),\n                \"fc2\": update_lambda(metric_model.fc2.weight)\n            }\n\n            loss = loss_fn(logits, labels, lambdas)\n\n            if torch.isnan(loss):\n                print(\"NaN loss encountered. Skipping batch.\")\n                continue\n\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n\n        # ---- Validation ----\n        siamese_merge.eval()\n        metric_model.eval()\n        total_val_loss = 0.0\n\n        with torch.no_grad():\n            for img1, img2, labels in tqdm(val_loader, desc=\"Validating\"):\n                img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n                merged_feats = siamese_merge(img1, img2)\n                logits = metric_model(merged_feats)\n\n                lambdas = {\n                    \"fc1\": update_lambda(metric_model.fc1.weight),\n                    \"fc2\": update_lambda(metric_model.fc2.weight)\n                }\n\n                val_loss = loss_fn(logits, labels, lambdas)\n                if not torch.isnan(val_loss):\n                    total_val_loss += val_loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n        \n        # --- Early Stopping ---\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            print(\"Best model updated.\")\n            torch.save({\n                'siamese_merge_state_dict': siamese_merge.state_dict(),\n                'metric_model_state_dict': metric_model.state_dict()\n            }, \"best_model.pth\")\n        else:\n            patience_counter += 1\n            print(f\"Patience: {patience_counter}/{patience}\")\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:49.509157Z","iopub.execute_input":"2025-04-19T12:08:49.509824Z","iopub.status.idle":"2025-04-19T12:08:49.518596Z","shell.execute_reply.started":"2025-04-19T12:08:49.509801Z","shell.execute_reply":"2025-04-19T12:08:49.517894Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsiamese_merge = SiameseMerge().to(device)\nmetric_model = MetricNetwork().to(device)\n\nloss_fn = MAPLoss(metric_model, siamese_merge.vgg).to(device)\n\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, \n           list(siamese_merge.parameters()) + list(metric_model.parameters())),\n    lr=1e-4, betas=(0.9, 0.999)\n)\n\ntrain_model(\n    siamese_merge=siamese_merge,\n    metric_model=metric_model,\n    loss_fn=loss_fn,\n    optimizer=optimizer,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    device=device,\n    num_epochs=20,\n    patience=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:08:53.134467Z","iopub.execute_input":"2025-04-19T12:08:53.135015Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5659/5659 [54:55<00:00,  1.72it/s]\nValidating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1403/1403 [21:19<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6479 | Val Loss: 0.4378\nBest model updated.\n\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5659/5659 [54:55<00:00,  1.72it/s]\nValidating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1403/1403 [21:17<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4296 | Val Loss: 0.4159\nBest model updated.\n\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 2212/5659 [21:23<33:55,  1.69it/s]","output_type":"stream"}],"execution_count":null}]}